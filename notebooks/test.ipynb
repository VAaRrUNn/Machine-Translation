{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 4\n",
    "idx = 0\n",
    "num_sentences = 2\n",
    "decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "\n",
    "encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "NEG_INFTY = -1e9\n",
    "\n",
    "eng_sentence_length, kn_sentence_length = 2, 2\n",
    "\n",
    "eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
    "kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n",
    "encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
    "encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
    "decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
    "decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
    "decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
    "decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
    "\n",
    "\n",
    "encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False,  True],\n",
       "         [ True,  True,  True,  True]],\n",
       "\n",
       "        [[False, False, False, False],\n",
       "         [False, False, False, False],\n",
       "         [False, False, False, False],\n",
       "         [False, False, False, False]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_chars_to_padding_mask, kn_chars_to_padding_mask\n",
    "encoder_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09],\n",
       "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_self_attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 X Longest X 64\n",
    "eg: 2 X 10 X 64\n",
    "\n",
    "att: 10 X 10  \n",
    "mask: 10 X 10 \n",
    "\n",
    "tokenser: 2 X 10 \n",
    "mask: 10 X 10 \n",
    "\n",
    "        my      name     is   PAD   PAD\n",
    "my      0       0        0    -inf  -inf\n",
    "name    0       0        0    -inf  -inf\n",
    "is      0       0        0    -inf  -inf\n",
    "PAD    -inf     -inf    -inf  -inf  -inf\n",
    "PAD    -inf     -inf    -inf  -inf  -inf\n",
    "\n",
    "tokenser: 2 X 10 \n",
    "emb: 2 X 10 X 64\n",
    "mask: 10 X 10 \n",
    "\n",
    "\n",
    "att_mask = 0 0 1\n",
    "0 0 1\n",
    "0 0 1\n",
    "1 1 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor([1, 2])\n",
    "for a in z:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.tensor([0, 0, 1])\n",
    "torch.where(s == 1, 10, 20)\n",
    "\n",
    "m_s = s.shape[0]\n",
    "mask = torch.zeros((m_s, m_s))\n",
    "for i in range(3):\n",
    "    if s[i] == 1:\n",
    "        mask[i, :] = -1\n",
    "        mask[:, i] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0., -1.],\n",
       "        [ 0.,  0., -1.],\n",
       "        [-1., -1., -1.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0., -1.],\n",
       "        [ 0.,  0., -1.],\n",
       "        [-1., -1., -1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_pad_mask(att):\n",
    "    s = len(att.tolist())\n",
    "    att_mask = torch.zeros((s, s))\n",
    "    for i in range(s):\n",
    "        if att[i] == 1:\n",
    "            att_mask[i, :] = -1\n",
    "            att_mask[:, i] = -1\n",
    "    return att_mask\n",
    "create_pad_mask(torch.tensor([0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((2, 10, 64))\n",
    "att = torch.randn((2, 10, 10))\n",
    "mask = torch.randn((10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 10]), torch.Size([10, 10]), torch.Size([2, 10, 10]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att.shape, mask.shape, (att + mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0],\n",
       "        [-1, -1, -1, -1, -1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((5, 5))\n",
    "m = (a == a[4])\n",
    "torch.where(m, -1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "               d_model: int,\n",
    "               ffn_hidden: int,\n",
    "               n_head: int,\n",
    "               drop_prob: int,\n",
    "               n_layers: int,\n",
    "               out_vocab_size: int):\n",
    "            super().__init__()\n",
    "            self.encoder = Encoder(d_model=d_model,\n",
    "                                ffn_hidden=ffn_hidden,\n",
    "                                n_head=n_head,\n",
    "                                drop_prob=drop_prob,\n",
    "                                n_layers=n_layers)\n",
    "            self.decoder = Decoder(d_model=d_model,\n",
    "                                ffn_hidden=ffn_hidden,\n",
    "                                n_head=n_head,\n",
    "                                drop_prob=drop_prob,\n",
    "                                n_layers=n_layers)\n",
    "            self.l = nn.Linear(d_model, out_vocab_size)\n",
    "\n",
    "    def forward(self,\n",
    "                enc_b,\n",
    "                dec_b,\n",
    "                enc_mask,\n",
    "                dec_mask):\n",
    "          enc_out = self.encoder(enc_b, enc_mask)\n",
    "          dec_out = self.decoder(dec_b, dec_mask, enc_out)\n",
    "          out = self.l(dec_out)\n",
    "          return out\n",
    "    \n",
    "trans = Transformer(d_model=d_model,\n",
    "            ffn_hidden=ffn_hidden,\n",
    "            n_head=n_head,\n",
    "            drop_prob=drop_prob,\n",
    "            n_layers=1,\n",
    "            out_vocab_size=10)\n",
    "trans(x, \n",
    "      x, \n",
    "      None,\n",
    "      None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = 10\n",
    "mask = torch.tril(torch.ones((L, L)))\n",
    "mask[mask == 0] = -torch.inf\n",
    "mask[mask == 1] = 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [2., 2., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [2., 2., 2., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [2., 2., 2., 2., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [2., 2., 2., 2., 2., -inf, -inf, -inf, -inf, -inf],\n",
       "        [2., 2., 2., 2., 2., 2., -inf, -inf, -inf, -inf],\n",
       "        [2., 2., 2., 2., 2., 2., 2., -inf, -inf, -inf],\n",
       "        [2., 2., 2., 2., 2., 2., 2., 2., -inf, -inf],\n",
       "        [2., 2., 2., 2., 2., 2., 2., 2., 2., -inf],\n",
       "        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask + mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
