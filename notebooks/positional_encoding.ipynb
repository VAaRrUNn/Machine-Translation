{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need positional Encodings?\n",
    "\n",
    "In other networks like RNN, LSTM's etc, they know about the loaction of the word cause the words are passed in order, whereas in Transformers like network, the words are passed in parallel so the transformer doesn't know about the actual sequence, or flow of occurence of the words which we passed, so to add a sense of position to it, we have this. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b3dfd25d10>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "torch.manual_seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 10\n",
    "d_model = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18.]),\n",
       " tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19.]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_d_p = torch.arange(0, d_model, 2).float()\n",
    "o_d_p = torch.arange(1, d_model, 2).float()\n",
    "e_d_p, o_d_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000e+00, 2.5119e+00, 6.3096e+00, 1.5849e+01, 3.9811e+01, 1.0000e+02,\n",
       "         2.5119e+02, 6.3096e+02, 1.5849e+03, 3.9811e+03]),\n",
       " tensor([1.0000e+00, 2.5119e+00, 6.3096e+00, 1.5849e+01, 3.9811e+01, 1.0000e+02,\n",
       "         2.5119e+02, 6.3096e+02, 1.5849e+03, 3.9811e+03]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_d = torch.pow(10000, e_d_p/d_model)\n",
    "o_d = torch.pow(10000, (o_d_p-1)/d_model)\n",
    "e_d, o_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = torch.arange(0, max_seq_len).reshape(-1, 1)\n",
    "even_pos = torch.sin(pos/ e_d.reshape(1, -1))\n",
    "odd_pos = torch.cos(pos/ o_d.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 10]),\n",
       " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 8.4147e-01,  3.8767e-01,  1.5783e-01,  6.3054e-02,  2.5116e-02,\n",
       "           9.9998e-03,  3.9811e-03,  1.5849e-03,  6.3096e-04,  2.5119e-04],\n",
       "         [ 9.0930e-01,  7.1471e-01,  3.1170e-01,  1.2586e-01,  5.0217e-02,\n",
       "           1.9999e-02,  7.9621e-03,  3.1698e-03,  1.2619e-03,  5.0238e-04],\n",
       "         [ 1.4112e-01,  9.2997e-01,  4.5775e-01,  1.8816e-01,  7.5285e-02,\n",
       "           2.9995e-02,  1.1943e-02,  4.7547e-03,  1.8929e-03,  7.5357e-04],\n",
       "         [-7.5680e-01,  9.9977e-01,  5.9234e-01,  2.4971e-01,  1.0031e-01,\n",
       "           3.9989e-02,  1.5924e-02,  6.3395e-03,  2.5238e-03,  1.0048e-03],\n",
       "         [-9.5892e-01,  9.1320e-01,  7.1207e-01,  3.1027e-01,  1.2526e-01,\n",
       "           4.9979e-02,  1.9904e-02,  7.9244e-03,  3.1548e-03,  1.2559e-03],\n",
       "         [-2.7942e-01,  6.8379e-01,  8.1396e-01,  3.6960e-01,  1.5014e-01,\n",
       "           5.9964e-02,  2.3884e-02,  9.5092e-03,  3.7857e-03,  1.5071e-03],\n",
       "         [ 6.5699e-01,  3.4744e-01,  8.9544e-01,  4.2745e-01,  1.7493e-01,\n",
       "           6.9943e-02,  2.7864e-02,  1.1094e-02,  4.4167e-03,  1.7583e-03],\n",
       "         [ 9.8936e-01, -4.3251e-02,  9.5448e-01,  4.8360e-01,  1.9960e-01,\n",
       "           7.9915e-02,  3.1843e-02,  1.2679e-02,  5.0476e-03,  2.0095e-03],\n",
       "         [ 4.1212e-01, -4.2718e-01,  9.8959e-01,  5.3783e-01,  2.2415e-01,\n",
       "           8.9879e-02,  3.5822e-02,  1.4264e-02,  5.6786e-03,  2.2607e-03]]),\n",
       " tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "           1.0000,  1.0000],\n",
       "         [ 0.5403,  0.9218,  0.9875,  0.9980,  0.9997,  0.9999,  1.0000,  1.0000,\n",
       "           1.0000,  1.0000],\n",
       "         [-0.4161,  0.6994,  0.9502,  0.9920,  0.9987,  0.9998,  1.0000,  1.0000,\n",
       "           1.0000,  1.0000],\n",
       "         [-0.9900,  0.3676,  0.8891,  0.9821,  0.9972,  0.9996,  0.9999,  1.0000,\n",
       "           1.0000,  1.0000],\n",
       "         [-0.6536, -0.0216,  0.8057,  0.9683,  0.9950,  0.9992,  0.9999,  1.0000,\n",
       "           1.0000,  1.0000],\n",
       "         [ 0.2837, -0.4075,  0.7021,  0.9506,  0.9921,  0.9988,  0.9998,  1.0000,\n",
       "           1.0000,  1.0000],\n",
       "         [ 0.9602, -0.7297,  0.5809,  0.9292,  0.9887,  0.9982,  0.9997,  1.0000,\n",
       "           1.0000,  1.0000],\n",
       "         [ 0.7539, -0.9377,  0.4452,  0.9040,  0.9846,  0.9976,  0.9996,  0.9999,\n",
       "           1.0000,  1.0000],\n",
       "         [-0.1455, -0.9991,  0.2983,  0.8753,  0.9799,  0.9968,  0.9995,  0.9999,\n",
       "           1.0000,  1.0000],\n",
       "         [-0.9111, -0.9042,  0.1439,  0.8431,  0.9746,  0.9960,  0.9994,  0.9999,\n",
       "           1.0000,  1.0000]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_pos.shape, even_pos, odd_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "PE = torch.stack([even_pos, odd_pos], dim=2)\n",
    "PE = torch.flatten(PE, start_dim=1, end_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 max_seq_len: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self) -> torch.tensor:\n",
    "        pos = torch.arange(0, self.max_seq_len)\n",
    "        denominator = torch.arange(0, self.d_model, 2)\n",
    "        denominator = torch.pow(10_000, denominator/self.d_model)\n",
    " \n",
    "        pos = pos.reshape(-1, 1)\n",
    "        denominator = denominator.reshape(1, -1)\n",
    "        even_pos = torch.sin(pos/ denominator)\n",
    "        odd_pos = torch.cos(pos/ denominator)\n",
    "\n",
    "        PE = torch.stack([even_pos, odd_pos], dim=2)\n",
    "        PE = torch.flatten(PE, start_dim=1, end_dim=2)\n",
    "        return PE\n",
    "    \n",
    "x = PositionalEncoding(d_model=20,\n",
    "                       max_seq_len=10)\n",
    "PE = x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
