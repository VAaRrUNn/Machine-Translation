{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpnsEMpdOlFx"
      },
      "source": [
        "## Installing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SQ-bchKOneG",
        "outputId": "361f8d21-7066-4a60-e155-21ed2d0b9a1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Lv6OgUhFEy-S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "MAX_SEQ_LEN = 300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6oA4j7xOBRf"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZOuH7OyOC3w",
        "outputId": "bd0dcdef-1d53-4f7d-8381-01f558f760ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
            "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 1659083\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 520\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 2507\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"cfilt/iitb-english-hindi\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRviST6LOC-g",
        "outputId": "8cb4f093-4876-430a-e028-fb5f0f11f299"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'en': 'Give your application an accessibility workout',\n",
              "  'hi': 'अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें'},\n",
              " {'en': 'Accerciser Accessibility Explorer',\n",
              "  'hi': 'एक्सेर्साइसर पहुंचनीयता अन्वेषक'},\n",
              " {'en': 'The default plugin layout for the bottom panel',\n",
              "  'hi': 'निचले पटल के लिए डिफोल्ट प्लग-इन खाका'},\n",
              " {'en': 'The default plugin layout for the top panel',\n",
              "  'hi': 'ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका'},\n",
              " {'en': 'A list of plugins that are disabled by default',\n",
              "  'hi': 'उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से निष्क्रिय किया गया है'},\n",
              " {'en': 'Highlight duration', 'hi': 'अवधि को हाइलाइट रकें'},\n",
              " {'en': 'The duration of the highlight box when selecting accessible nodes',\n",
              "  'hi': 'पहुंचनीय आसंधि (नोड) को चुनते समय हाइलाइट बक्से की अवधि'},\n",
              " {'en': 'Highlight border color',\n",
              "  'hi': 'सीमांत (बोर्डर) के रंग को हाइलाइट करें'},\n",
              " {'en': 'The color and opacity of the highlight border.',\n",
              "  'hi': 'हाइलाइट किए गए सीमांत का रंग और अपारदर्शिता। '},\n",
              " {'en': 'Highlight fill color', 'hi': 'भराई के रंग को हाइलाइट करें'}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"]['translation'][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_MMgxl6JODFs"
      },
      "outputs": [],
      "source": [
        "d = dataset[\"train\"][\"translation\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEWE1Z4IS2eb",
        "outputId": "123f238b-ab1c-454a-b1cf-dd380e9dc7a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1659083"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"].num_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXElTqsEQvJf",
        "outputId": "3631a8ed-9e50-4fb2-cc4f-61f3d7109252"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'en': 'Give your application an accessibility workout',\n",
              " 'hi': 'अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPUGvp9HSe11",
        "outputId": "d4e31d18-aeaf-4f66-d8b8-bacbf9d97a6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "datasets.dataset_dict.DatasetDict"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ0yun3oFrt8"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kA55bxQpQvPz"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "  def __init__(self,\n",
        "               dataset: datasets.dataset_dict.DatasetDict,\n",
        "               split: str = \"train\",\n",
        "               return_only: str = None):\n",
        "    self.data = dataset[split][\"translation\"]\n",
        "    self.return_only = return_only\n",
        "\n",
        "  def __len__(self,):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self,\n",
        "                  idx: int):\n",
        "    en, hi = self.data[idx][\"en\"], self.data[idx][\"hi\"]\n",
        "    if self.return_only == \"en\":\n",
        "      return en\n",
        "    elif self.return_only == \"hi\":\n",
        "      return hi\n",
        "    return en, hi\n",
        "\n",
        "nd = TextDataset(dataset=dataset,\n",
        "                 split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH7DtTpCTQr1",
        "outputId": "8ffa1fd5-396b-4ba6-eb60-8076962f5142"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Give your application an accessibility workout',\n",
              " 'अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nd[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZLlkxvrix9L"
      },
      "source": [
        "## Training a tokenizer for English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Es0GOg7qQyy"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "q4_kIQSOiyDF"
      },
      "outputs": [],
      "source": [
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import NFD, StripAccents\n",
        "normalizer = normalizers.Sequence([StripAccents()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MHhPjFs-iyJL",
        "outputId": "d3005c06-19ac-4425-e6c3-33b63b5e6292"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Script Recorder'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "normalizer.normalize_str(\"Script Recorder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5trFnKfNiyOk"
      },
      "source": [
        "### Pre-tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0agDAy_giyUP"
      },
      "outputs": [],
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "pre_tokenizer = Whitespace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSS3EMJyiyZx",
        "outputId": "e52e29ee-de8e-4d8a-b4b8-1aebd5149d0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Script', (0, 6)), ('Recorder', (7, 15))]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_tokenizer.pre_tokenize_str(\"Script Recorder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icCCFQTxiyfO"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VhryULKXiykX"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "eng_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "eng_tokenizer.normalizer = normalizer\n",
        "eng_tokenizer.pre_tokenizer = pre_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0WfYWO1QpyV",
        "outputId": "051bdbcb-708e-41b4-f70e-6448545bcb94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n"
          ]
        }
      ],
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "trainer = BpeTrainer(vocab_size=30_000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
        "eng_dataset = TextDataset(dataset=dataset,\n",
        "                          split=\"train\",\n",
        "                          return_only=\"en\")\n",
        "data = eng_dataset\n",
        "print(\"Training...\")\n",
        "# IG The progress bar only shows up in terminal not in notebook...\n",
        "eng_tokenizer.train_from_iterator(data, trainer=trainer, length=len(data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zq9bdyWFv4Q"
      },
      "source": [
        "## Saving the Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XUtwX3D7ajRP"
      },
      "outputs": [],
      "source": [
        "eng_tokenizer.save(\"eng_tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRNLeTofFyPO"
      },
      "source": [
        "## Custom decoder for english tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lj_zM645xLtc"
      },
      "outputs": [],
      "source": [
        "def reconstruct_text(encoded, prev_offset = 0, pad_token=\"[PAD]\"):\n",
        "    \"\"\"\n",
        "    Reconstructs the original input text\n",
        "    \"\"\"\n",
        "    if len(encoded.tokens) != len(encoded.offsets):\n",
        "        raise ValueError(\"Mismatched lengths between tokens and offsets\")\n",
        "\n",
        "    reconstructed_text = \"\"\n",
        "\n",
        "    for i in range(len(encoded.tokens)):\n",
        "        start, end = encoded.offsets[i]\n",
        "        token_text = encoded.tokens[i]\n",
        "        if token_text == pad_token:\n",
        "          return reconstructed_text\n",
        "        if start is not None and end is not None:\n",
        "            if start == prev_offset:\n",
        "              reconstructed_text += token_text\n",
        "            else:\n",
        "              reconstructed_text += ' ' + token_text\n",
        "        prev_offset = end\n",
        "\n",
        "    return reconstructed_text\n",
        "\n",
        "def reconstruct_text_batch(encoded_list, pad_token = \"[PAD]\"):\n",
        "  out = []\n",
        "  for encoded in encoded_list:\n",
        "    out.append(reconstruct_text(encoded, 0, pad_token))\n",
        "  return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz6720m5OrKa"
      },
      "source": [
        "## Checking English Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1rDV-rRF-d7"
      },
      "source": [
        "### Single sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPjKdfSkiypo",
        "outputId": "84cf2ea9-5bb0-4011-b255-061fab123f08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text Script Recorder\n",
            "tokens: ['Script', 'Record', 'er']\n",
            "Ids: [7348, 7617, 751]\n",
            "Reconstructed ids: Script Recorder\n",
            "Decoded ids: Script Record er\n"
          ]
        }
      ],
      "source": [
        "text = eng_dataset[100]\n",
        "eng_tokenizer = Tokenizer.from_file(\"eng_tokenizer.json\")\n",
        "out = eng_tokenizer.encode(text)\n",
        "\n",
        "print(f\"Original Text {text}\")\n",
        "print(f\"tokens: {out.tokens}\")\n",
        "print(f\"Ids: {out.ids}\")\n",
        "print(f\"Reconstructed ids: {reconstruct_text(out)}\")\n",
        "\n",
        "# Currently the decoder is not able to combine the ids to give the correct\n",
        "# input text\n",
        "\n",
        "# from tokenizers import decoders\n",
        "# eng_tokenizer.decoder = decoders.ByteLevel()\n",
        "\n",
        "decoded = eng_tokenizer.decode(out.ids)\n",
        "print(f\"Decoded ids: {decoded}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGXFQk7EGAeA"
      },
      "source": [
        "### Batched sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFud5oXAVdRe",
        "outputId": "aab4d529-11b3-4f01-9d4a-8bd88f5eacd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: ['The color and opacity of the highlight fill.', '_ Monitor Events']\n",
            "['The', 'color', 'and', 'opacity', 'of', 'the', 'highlight', 'fill', '.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "[799, 3485, 761, 23454, 756, 747, 8011, 2373, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "['_', 'Monitor', 'Events', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "[65, 8683, 18105, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Reconstructed ids:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['The color and opacity of the highlight fill.', '_ Monitor Events']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = [eng_dataset[10], eng_dataset[20]]\n",
        "# Enabling padding for batched sentences\n",
        "pad_id, unk_id = eng_tokenizer.token_to_id(\"[PAD]\"), eng_tokenizer.token_to_id(\"[UNK]\")\n",
        "eng_tokenizer.enable_padding(pad_token=\"[PAD]\", pad_id = pad_id, length = MAX_SEQ_LEN)\n",
        "\n",
        "o1 = eng_tokenizer.encode_batch(text)\n",
        "print(f\"Original: {text}\")\n",
        "for o in o1:\n",
        "  print(o.tokens)\n",
        "  print(o.ids)\n",
        "\n",
        "print(f\"Reconstructed ids:\")\n",
        "re = reconstruct_text_batch(o1)\n",
        "re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "37EF1152FMHg"
      },
      "outputs": [],
      "source": [
        "eng_tokenizer.save(\"eng_tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OmFNdUPiy0J"
      },
      "source": [
        "## Hindi Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0tRTt0Biy5D",
        "outputId": "d6749acf-b84d-4caa-932a-06b45dc8662e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import NFD, StripAccents\n",
        "normalizer = normalizers.Sequence([NFD()])\n",
        "\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "pre_tokenizer = Whitespace()\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "hi_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "hi_tokenizer.normalizer = normalizer\n",
        "hi_tokenizer.pre_tokenizer = pre_tokenizer\n",
        "\n",
        "\n",
        "# Training\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "trainer = WordPieceTrainer(vocab_size=60_000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
        "hi_dataset = TextDataset(dataset=dataset,\n",
        "                          split=\"train\",\n",
        "                          return_only=\"hi\")\n",
        "data = hi_dataset\n",
        "print(\"Training...\")\n",
        "hi_tokenizer.train_from_iterator(data, trainer=trainer, length=len(data))\n",
        "\n",
        "hi_tokenizer.save(\"hi_tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W7xGshtGHDr"
      },
      "source": [
        "## Testing Hindi Tokenizer for Batched sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sfGQEjQmDVFn"
      },
      "outputs": [],
      "source": [
        "text = [hi_dataset[100], hi_dataset[101]]\n",
        "hi_tokenizer = Tokenizer.from_file(\"hi_tokenizer.json\")\n",
        "\n",
        "# Enabling dynamic padding for batched sentences\n",
        "pad_id, unk_id = hi_tokenizer.token_to_id(\"[PAD]\"), hi_tokenizer.token_to_id(\"[UNK]\")\n",
        "hi_tokenizer.enable_padding(pad_token=\"[PAD]\", pad_id = pad_id, length = MAX_SEQ_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB1gWifwMcfZ",
        "outputId": "fea4e003-bf49-4f1a-a017-59dd0fa331c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text ['लिपि रेकोर्डर', 'श्वानपुच्छ शैली की लिपियां निर्मित करता है']\n",
            "Output of the tokenizer\n",
            "['लिपि', 'रेक', '##ोर्ड', '##र', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "[7288, 21256, 25157, 467, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "300\n",
            "['श्वान', '##पुच्छ', 'शैली', 'की', 'लिपि', '##यां', 'निर्मित', 'करता', 'है', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "[36178, 30383, 3338, 808, 7288, 5118, 3505, 1086, 796, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "300\n",
            "Decoded tokens....\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['लिपि रेकोर्डर', 'श्वानपुच्छ शैली की लिपियां निर्मित करता है']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tokenizers import decoders\n",
        "hi_tokenizer.decoder = decoders.WordPiece()\n",
        "\n",
        "out = hi_tokenizer.encode_batch(text, add_special_tokens=False)\n",
        "print(f\"Original text {text}\")\n",
        "print(f\"Output of the tokenizer\")\n",
        "for os in out:\n",
        "  print(os.tokens)\n",
        "  print(os.ids)\n",
        "  print(len(os.ids))\n",
        "\n",
        "print(f\"Decoded tokens....\")\n",
        "decoded = hi_tokenizer.decode_batch([o.ids for o in out])\n",
        "decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LhWVHSW_E1IM"
      },
      "outputs": [],
      "source": [
        "# Do this to check for available methods\n",
        "# help(hi_tokenizer)\n",
        "# or this to be more precise\n",
        "# help(hi_tokenizer.decode_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qssmPXyFOgfr"
      },
      "outputs": [],
      "source": [
        "hi_tokenizer.save(\"hi_tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECPZpdeOO9a0"
      },
      "source": [
        "## Common Utilities For Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "TyrH6EtpNgxe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 max_seq_len: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def forward(self) -> torch.tensor:\n",
        "        pos = torch.arange(0, self.max_seq_len)\n",
        "        denominator = torch.arange(0, self.d_model, 2)\n",
        "        denominator = torch.pow(10_000, denominator/self.d_model)\n",
        "\n",
        "        pos = pos.reshape(-1, 1)\n",
        "        denominator = denominator.reshape(1, -1)\n",
        "        even_pos = torch.sin(pos / denominator)\n",
        "        odd_pos = torch.cos(pos / denominator)\n",
        "\n",
        "        PE = torch.stack([even_pos, odd_pos], dim=2)\n",
        "        PE = torch.flatten(PE, start_dim=1, end_dim=2)\n",
        "        return PE\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 d_model: int,\n",
        "                 n_head: int):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.n_head = n_head\n",
        "        self.h_dim = d_model // n_head\n",
        "        self.qkv_layer = nn.Linear(input_dim, 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.tensor,\n",
        "                mask: torch.tensor = None):\n",
        "        B, sen_len, input_dim = x.size()\n",
        "        qkv = self.qkv_layer(x)  # B, sen_len, 3 * d_model\n",
        "        qkv = qkv.reshape(B, sen_len, self.n_head, self.h_dim * 3)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        d_k = q.size()[-1]\n",
        "        att = (q @ k.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k))\n",
        "        if mask is not None:\n",
        "            # print(f\"mask: {mask.shape}\")\n",
        "            # print(f\"att: {att.shape}\")\n",
        "            # changing mask shape:\n",
        "            mask = mask.unsqueeze(1).expand_as(att)\n",
        "            att += mask\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        new_emb = att @ v\n",
        "        new_emb = new_emb.reshape(B, sen_len, self.n_head * self.h_dim)\n",
        "        new_emb = self.linear_layer(new_emb)\n",
        "        return att, new_emb\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape = parameters_shape\n",
        "        self.eps = eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, x):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = x.mean(dim=dims, keepdim=True)\n",
        "        var = ((x - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        y = (x - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 ffn_hidden: int,\n",
        "                 drop_prob: float):\n",
        "        super().__init__()\n",
        "        self.l = nn.Sequential(\n",
        "            nn.Linear(d_model, ffn_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop_prob),\n",
        "            nn.Linear(ffn_hidden, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop_prob),\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "                x):\n",
        "        out = self.l(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 n_head: int):\n",
        "\n",
        "        super().__init__()\n",
        "        self.q_layer = nn.Linear(d_model, d_model)\n",
        "        self.k_layer = nn.Linear(d_model, d_model)\n",
        "        self.v_layer = nn.Linear(d_model, d_model)\n",
        "        self.n_head = n_head\n",
        "\n",
        "    def forward(self,\n",
        "                enc_out: torch.tensor,\n",
        "                dec_out: torch.tensor):\n",
        "\n",
        "        B, max_sen_len, d_model = enc_out.size()\n",
        "\n",
        "        q = self.q_layer(dec_out)\n",
        "        k = self.k_layer(enc_out)\n",
        "        v = self.v_layer(enc_out)\n",
        "\n",
        "        q = q.reshape(B, max_sen_len, self.n_head, d_model // self.n_head)\n",
        "        k = k.reshape(B, max_sen_len, self.n_head, d_model // self.n_head)\n",
        "        v = v.reshape(B, max_sen_len, self.n_head, d_model // self.n_head)\n",
        "\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_model))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        new_emb = att @ v\n",
        "        new_emb = new_emb.reshape(B, max_sen_len, self.n_head * (d_model // self.n_head))\n",
        "\n",
        "        return att, new_emb\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rae5w9OUO9gm"
      },
      "source": [
        "## Encoder and Decoder Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "eiHM2TDvNmB4"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 ffn_hidden: int,\n",
        "                 n_head: int,\n",
        "                 drop_prob: float):\n",
        "        super().__init__()\n",
        "        self.m_att = MultiHeadAttention(input_dim=d_model,\n",
        "                                        d_model=d_model,\n",
        "                                        n_head=n_head)\n",
        "        self.l_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.ffn = FeedForwardNetwork(d_model=d_model,\n",
        "                                      ffn_hidden=ffn_hidden,\n",
        "                                      drop_prob=drop_prob)\n",
        "        self.l_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "\n",
        "    def forward(self,\n",
        "                x,\n",
        "                mask=None):\n",
        "        _, att = self.m_att(x, mask)\n",
        "        att = self.l_norm1(att + x)\n",
        "\n",
        "        out = self.ffn(att)\n",
        "        out = self.l_norm2(att + out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class SequentialEncoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, self_attention_mask  = inputs\n",
        "        for module in self._modules.values():\n",
        "            x = module(x, self_attention_mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 ffn_hidden: int,\n",
        "                 n_head: int,\n",
        "                 drop_prob: float,\n",
        "                 n_layers: int):\n",
        "        super().__init__()\n",
        "        self.l = SequentialEncoder(*[EncoderLayer(d_model=d_model,\n",
        "                                              ffn_hidden=ffn_hidden,\n",
        "                                              n_head=n_head,\n",
        "                                              drop_prob=drop_prob) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.tensor,\n",
        "                mask = None) -> torch.tensor:\n",
        "        out = self.l(x, mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 ffn_hidden: int,\n",
        "                 n_head: int,\n",
        "                 drop_prob: float):\n",
        "        super().__init__()\n",
        "        self.masked_att = MultiHeadAttention(input_dim=d_model,\n",
        "                                        d_model=d_model,\n",
        "                                        n_head=n_head)\n",
        "        self.ffn = FeedForwardNetwork(d_model=d_model,\n",
        "                                      ffn_hidden=ffn_hidden,\n",
        "                                      drop_prob=drop_prob)\n",
        "        self.l_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.l_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.l_norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "\n",
        "\n",
        "        self.mcross_att = MultiHeadCrossAttention(d_model=d_model,\n",
        "                                                  n_head=n_head)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.tensor,\n",
        "                mask: torch.tensor,\n",
        "                enc_out: torch.tensor):\n",
        "\n",
        "        _, att = self.masked_att(x, mask)\n",
        "        att = self.l_norm1(att + x)\n",
        "\n",
        "        _, out = self.mcross_att(enc_out, att)\n",
        "        out = self.l_norm2(out + att)\n",
        "\n",
        "        f_out = self.ffn(out)\n",
        "        out = self.l_norm2(f_out + out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class SequentialDecoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, mask, y = inputs\n",
        "        for module in self._modules.values():\n",
        "            y = module(x, mask, y) #30 x 200 x 512\n",
        "        return y\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 ffn_hidden: int,\n",
        "                 n_head: int,\n",
        "                 drop_prob: float,\n",
        "                 n_layers: int):\n",
        "        super().__init__()\n",
        "        self.l = SequentialDecoder(*[DecoderLayer(d_model=d_model,\n",
        "                                              ffn_hidden=ffn_hidden,\n",
        "                                              n_head=n_head,\n",
        "                                              drop_prob=drop_prob) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.tensor,\n",
        "                mask: torch.tensor,\n",
        "                enc_out: torch.tensor) -> torch.tensor:\n",
        "        out = self.l(x, mask, enc_out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebJ4uqjsNmF_"
      },
      "source": [
        "## Simple testing of Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiwL0mccNmKx",
        "outputId": "60af7baa-29a0-4ff7-b3e7-e435c53a9350"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "n_heads = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 32\n",
        "max_seq_len = 200\n",
        "ffn_hidden = 2048\n",
        "n_layers = 5\n",
        "x = torch.randn( (batch_size, max_seq_len, d_model))\n",
        "\n",
        "dec = Decoder(d_model,\n",
        "             ffn_hidden,\n",
        "             n_heads,\n",
        "             drop_prob,\n",
        "             1)\n",
        "# mask = torch.full([max_seq_len, max_seq_len] , float('-inf'))\n",
        "# mask = torch.triu(mask, diagonal=1)\n",
        "print(dec(x=x, enc_out=x, mask=None).shape)\n",
        "\n",
        "enc = EncoderLayer(d_model,\n",
        "             ffn_hidden,\n",
        "             n_heads,\n",
        "             drop_prob)\n",
        "print(enc(x).shape)\n",
        "\n",
        "enc = Encoder(d_model,\n",
        "             ffn_hidden,\n",
        "             n_heads,\n",
        "             drop_prob,\n",
        "              2)\n",
        "print(enc(x).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FmT5CM2O9li"
      },
      "source": [
        "## Masks for Encoder and Decoder (Padding mask and Look ahead mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9lubOsAhQyct"
      },
      "outputs": [],
      "source": [
        "def create_pad_mask(att_mask, pad_token_id = 1, pad_item = 1e-9):\n",
        "  \"\"\"\n",
        "  Gives padding mask according to att_mask\n",
        "\n",
        "  Args:\n",
        "    att_mask: list of token.ids\n",
        "    pad_token_id: Padding token id\n",
        "    pad_item: Value to fill in the mask for padding token\n",
        "  \"\"\"\n",
        "  s = len(att_mask)\n",
        "  att_mask = torch.tensor(att_mask)\n",
        "  pad_mask = torch.zeros((s, s))\n",
        "  for i in range(s):\n",
        "    if att_mask[i] == pad_token_id:\n",
        "      pad_mask[i, :] = pad_item\n",
        "      pad_mask[:, i] = pad_item\n",
        "  return pad_mask\n",
        "\n",
        "def create_pad_mask_b(att_masks,\n",
        "                      pad_token_id = 1,\n",
        "                      pad_item = 1e-9):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZN6NR-5qUM3"
      },
      "source": [
        "\n",
        "## Creating dataloaders, masks, inputs, outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "l0SVWmDcQyiC"
      },
      "outputs": [],
      "source": [
        "# Creating datasets\n",
        "# They will return a single sentence\n",
        "en_dataset = TextDataset(dataset=dataset,\n",
        "                         split=\"train\",\n",
        "                         return_only=\"en\")\n",
        "\n",
        "hi_dataset = TextDataset(dataset=dataset,\n",
        "                         split=\"train\",\n",
        "                         return_only=\"hi\")\n",
        "\n",
        "batch_size = 2\n",
        "def encoder_collate_fn(x, tokenizer):\n",
        "  \"\"\"\n",
        "  x: List of sentences\n",
        "  \"\"\"\n",
        "  encodings = tokenizer.encode_batch(x)\n",
        "  L = len(encodings[0].ids)\n",
        "  input_tensor = torch.zeros((len(x), L), dtype=torch.long)\n",
        "  mask = torch.zeros((len(x), L, L))\n",
        "  for i, enc in enumerate(encodings):\n",
        "    input_tensor[i] = torch.tensor(enc.ids)\n",
        "    pad_mask = create_pad_mask(enc.ids)\n",
        "    mask[i] = pad_mask\n",
        "\n",
        "  return input_tensor, mask\n",
        "\n",
        "\n",
        "def decoder_collate_fn(x, tokenizer):\n",
        "  \"\"\"\n",
        "  x: List of sentences\n",
        "  \"\"\"\n",
        "  encodings = tokenizer.encode_batch(x)\n",
        "  L = len(encodings[0].ids)\n",
        "  input_tensor = torch.zeros((len(x), L), dtype=torch.long)\n",
        "  mask = torch.zeros((len(x), L, L))\n",
        "\n",
        "  look_ahead_mask = torch.tril(torch.ones(L, L))\n",
        "  look_ahead_mask[look_ahead_mask == 0] = -torch.inf\n",
        "  look_ahead_mask[look_ahead_mask == 1] = 0\n",
        "\n",
        "  for i, enc in enumerate(encodings):\n",
        "    input_tensor[i] = torch.tensor(enc.ids)\n",
        "    pad_mask = create_pad_mask(enc.ids)\n",
        "    mask[i] = pad_mask + look_ahead_mask\n",
        "\n",
        "  return input_tensor, mask\n",
        "\n",
        "# Tokenizers..\n",
        "eng_tokenizer = Tokenizer.from_file(\"eng_tokenizer.json\")\n",
        "hi_tokenizer = Tokenizer.from_file(\"hi_tokenizer.json\")\n",
        "\n",
        "# Creating dataloaderss\n",
        "# They will return respective input tensor with their masks.\n",
        "en_train_loader = DataLoader(dataset=en_dataset, batch_size=batch_size, collate_fn = lambda b: encoder_collate_fn(b, eng_tokenizer))\n",
        "hi_train_loader = DataLoader(dataset=hi_dataset, batch_size=batch_size, collate_fn = lambda b: decoder_collate_fn(b, hi_tokenizer))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMbvyWJmQyoN"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "YqbxZPKZQyt2"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "               d_model: int,\n",
        "               ffn_hidden: int,\n",
        "               n_head: int,\n",
        "               drop_prob: int,\n",
        "               n_layers: int,\n",
        "               hi_vocab_size: int):\n",
        "            super().__init__()\n",
        "\n",
        "            self.encoder = Encoder(d_model=d_model,\n",
        "                                ffn_hidden=ffn_hidden,\n",
        "                                n_head=n_head,\n",
        "                                drop_prob=drop_prob,\n",
        "                                n_layers=n_layers)\n",
        "            self.decoder = Decoder(d_model=d_model,\n",
        "                                ffn_hidden=ffn_hidden,\n",
        "                                n_head=n_head,\n",
        "                                drop_prob=drop_prob,\n",
        "                                n_layers=n_layers)\n",
        "            self.l = nn.Linear(d_model, hi_vocab_size)\n",
        "\n",
        "    def forward(self,\n",
        "                enc_b,\n",
        "                dec_b,\n",
        "                enc_mask,\n",
        "                dec_mask):\n",
        "          enc_out = self.encoder(enc_b, enc_mask)\n",
        "          dec_out = self.decoder(dec_b, dec_mask, enc_out)\n",
        "          out = self.l(dec_out)\n",
        "          return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx2ZwOWtGqNW"
      },
      "source": [
        "## Testing Transformer Model for a Dry run..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "nsdv67VNYWeu"
      },
      "outputs": [],
      "source": [
        "class MachineTranslation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 n_head,\n",
        "                 drop_prob,\n",
        "                 n_layers,\n",
        "                 eng_vocab_size,\n",
        "                 hi_vocab_size):\n",
        "\n",
        "        super().__init__()\n",
        "        self.transformer = Transformer(d_model=d_model,\n",
        "                                       ffn_hidden=ffn_hidden,\n",
        "                                       n_head=n_head,\n",
        "                                       drop_prob=drop_prob,\n",
        "                                       n_layers=1,\n",
        "                                       hi_vocab_size=hi_vocab_size)\n",
        "\n",
        "        self.pos_enc = PositionalEncoding(d_model, MAX_SEQ_LEN)\n",
        "        self.emb = nn.Embedding(eng_vocab_size, d_model)\n",
        "\n",
        "    def forward(self,\n",
        "                enc_b,\n",
        "                dec_b,\n",
        "                enc_mask,\n",
        "                dec_mask):\n",
        "        enc_b = self.emb(enc_b) + self.pos_enc()\n",
        "        print(f\"enc_b {enc_b.shape}\")\n",
        "        print(f\"dec_b {dec_b.shape}\")\n",
        "        print(f\"enc_mask {enc_mask.shape}\")\n",
        "        print(f\"dec_mask {dec_mask.shape}\")\n",
        "        out = self.transformer(enc_b,\n",
        "                          dec_b,\n",
        "                          enc_mask,\n",
        "                          dec_mask)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "nPv8PRryQyzq",
        "outputId": "255ba556-2160-4a9b-da22-aa3872f751d8"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "n_head = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 32\n",
        "max_seq_len = 200\n",
        "ffn_hidden = 2048\n",
        "n_layers = 5\n",
        "\n",
        "eng_vocab_size = eng_tokenizer.get_vocab_size()\n",
        "hi_vocab_size = hi_tokenizer.get_vocab_size()\n",
        "\n",
        "trans = MachineTranslation(d_model=d_model,\n",
        "            ffn_hidden=ffn_hidden,\n",
        "            n_head=n_head,\n",
        "            drop_prob=drop_prob,\n",
        "            n_layers=1,\n",
        "            eng_vocab_size = eng_vocab_size,\n",
        "            hi_vocab_size=hi_vocab_size)\n",
        "\n",
        "eng_sen, enc_mask = next(iter(en_train_loader))\n",
        "hi_sen, dec_mask = next(iter(hi_train_loader))\n",
        "\n",
        "\n",
        "print(f\"Passing english sentences through the transformer\")\n",
        "out = trans(eng_sen,\n",
        "      eng_sen,\n",
        "      enc_mask,\n",
        "      dec_mask)\n",
        "\n",
        "print(f\"English sentences processeed\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optim = torch.optim.Adam(params = trans.parameters())\n",
        "loss = criterion(out.view(-1, hi_vocab_size),\n",
        "                 hi_sen.view(-1))\n",
        "print(f\"Loss: {loss.item()}\")\n",
        "print(f\"Backprop...\")\n",
        "loss.backward()\n",
        "print(f\"Done...\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
